---
title: "Statistical Inference"
author: "Joseph Walker"
date: "3/7/2019"
output: html_document
---

Welcome to this post on Statistical Inference. 

When we have a problem or question we're trying to answer, it is often impractical or even impossible to gather data on the entire population (political polling data, marketing products, water quality sampling, etc...). **Statistical Inference is a process in which we make conclusions about a population based on a sample from the data.** It draws upon hypothesis testing to make these claims.

The idea behind statistical inference is to understand samples from a hypothetical population in which the Null hypothesis (H~o~), the claim that is not interesting, is true. Most of the time, the goal is to *disprove the null hypothesis* in favor of the Alternative hypothesis (H~a~), the claim corresonding to the question or problem in research.

Let's use a more concrete example. Using the `cancer.in.dogs` datset from the `openintro` package, we want to know whether exposure to the herbicide 2,4-dichlorophenoxyacetic acid (2,4-D) increased the risk of cancer in dogs.

The H~o~ is: There is no relationship between cancer in dogs and exposure to the herbicide.  
The H~a~ is: Dogs exposed to 2,4-D are more likely to have cancer than dogs not exposed to the herbicide.


```{r}
#load libraries
library(tidyverse)
library(openintro)

#examine the data
table(cancer.in.dogs)
```

This is quite a simple dataset. In the next steps, we will use the `infer` package to model the Null hypothesis and randomize the data to calculate permuted statistics. Permuting the data in this fashion will ensure there is no relationship between the two variables.

```{r}
#load infer package
library(infer)

#specify the model
cid_perm <- cancer.in.dogs %>%
  specify(response ~ order, success = "cancer") %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "diff in props", order = c("2,4-D", "no 2,4-D"))

#visualize the results
ggplot(cid_perm, aes(x = stat)) +
  geom_dotplot(binwidth = .001)
```

To summarize, we have *specified* our model terms (response ~ order), declared the *hypothesis* that null is true where response and order are not related, *permuted* the data 1000 times, and calculated the differenice in proportions for each of these permuation sets. This leaves us with the dotplot of showing the distribution of differences in proportions for each of the 1000 permutations.

Now let's calculate the difference in proportions for the actual dataset.

*Important to note, it is also possible to use other statistics, such as the ratio of the proportions, to investigate the relationship between the Null and observed statistics.*

```{r}
actual <- cancer.in.dogs %>%   
  # Group by 
  group_by(order) %>%
  # Summarize proportion of dogs that have cancer
  summarise(prop_cancer = mean(response == "cancer")) %>%
  # Summarize difference in proportion of dogs with cancer that were exposed vs. not exposed
  summarise(obs_diff_prop = diff(prop_cancer,differences = 0 )) # "2,4-D" - no "2,4-D"
  
# See the result
actual
```

Finally we'll combine the permuted data with the observed data.
```{r}
cid_perm <- cid_perm %>%
  mutate(actual_diff = actual$obs_diff_prop)

# Plot permuted differences
ggplot(cid_perm, aes(x = stat)) + 
  geom_dotplot(binwidth = .001) +
  geom_vline(aes(xintercept = actual_diff), color = "red")
 
# Compare permuted differences to observed difference
cid_perm %>%
  summarize(sum(actual_diff >= stat))

```

Out of the 1000 permutations, only 5 were more extreme (less) than our actual observation. In the event the Null hypothesis were true, the likelihood of permuted data similar to the observed data would be greater, however, this is not the case. The observed data is not consistent with the null statistics. We'll come back to this disagreement, but first, it's important to understand more about these extreme permuted differences.

The *extreme* permuted differences, that is, the permuted differences which lie on the tails of the distribution under which the Null hypothesis is true, are also known as the **critical region**. We can compute these using the quantile function.

```{r}
#find the .1, .05, and .01 quantiles
quants <- cid_perm %>% 
  summarize(q.1 = quantile(stat, p = .1),
            q.05 = quantile(stat, p = .05),
            q.01 = quantile(stat, p = .01))

#examine the results
quants
```

What these values tell us is how much of the underlying distribution falls below (or above) the observed stat. In other words, 5% of the values (50/1000) are less than `r quants$q.05`.

## Critical Region

Hypothesis testing can be one sided, or two sided. In a one-sided test, we only care about the relationship of the critical region of permuted observations and the actual observation on one of the tails. In a two-sided test, we want to capture the critical region on both tails.

So why is **.05** such a common cutoff for hypothesis testing? Well, it's actually quite arbitrary. The level of significance is a highly subjective matter which is dependent upon the researcher and the research. Whatever you choose as a signficance, the results should always lead to further investigation.

The sample size has an important effect on the critical region as we will explore here. We'll create two new datasets stemming from the `cancer.in.dogs` dataset we're already familiar with. One will be a subset of the original data to show the effect small sample sizes have, and the other will be larger.

```{r}
# set seed for reproducibility
set.seed(455)

#create small dataset - 30% of original data
cid_small <- sample_frac(cancer.in.dogs, size = .3, replace = T)

#create large dataset - 10x larger than original data
cid_large <- sample_frac(cancer.in.dogs, size = 10, replace = T)

#examine contingency tables for each dataset
map(list(cancer.in.dogs, cid_small, cid_large), table)
```

As before, let's caclulate the difference in proportions for each.

```{r}
diff_prop_function <- function(x){
  x %>%
    group_by(order) %>%
    summarise(prop_cancer = mean(response == "cancer")) %>%
    summarise(obs_diff_prop = diff(prop_cancer))
}

#diff in proportions for small dataset
actual_small <- diff_prop_function(cid_small)

#difference in proportions for large dataset
actual_large <- diff_prop_function(cid_large)
```

Now we need to create the permuted difference in proportions for each of the datasets.

```{r}
permuted_diff_function <- function(x){
  x %>%
    specify(response ~ order, success = "cancer") %>%
    hypothesize(null = "independence") %>%
    generate(reps = 1000, type = "permute") %>%
   calculate(stat = "diff in props", order = c("2,4-D", "no 2,4-D"))
}

#calculate permuted difference in props for small dataset
cid_perm_small <- permuted_diff_function(cid_small)

#calculate permuted difference in props for small dataset
cid_perm_large <- permuted_diff_function(cid_large)
```

Alright! Now it's time to visualize the effect sample size has on the distribution and the critical region.

```{r}
x <- list(small =cid_perm_small, original = cid_perm, large = cid_perm_large)
y <- list(actual_small$obs_diff_prop, actual$obs_diff_prop, actual_large$obs_diff_prop)

#iterate over the datasets
plots <- pmap(.l = list(x = x,y = y, z = names(x)),
     .f = function(x, y, z) ggplot(x, aes(x = stat)) +
       geom_histogram(binwidth = 0.01) +
       geom_vline(aes(xintercept = y), color = "red") +
       labs(title = z))

#print output
walk(plots, print)
```

As the dataset becomes larger, the likelihood of seeing the observed difference due to chance becomes smaller.

Now let's examine the critical regions by calculating the upper quantiles for each dataset. 
```{r}
#quantiles function
calc_lower_quantiles <- function(dataset) {
  dataset %>% 
    summarize(
      q.1 = quantile(stat, p = 0.1),
      q.05 = quantile(stat, p = 0.05),
      q.01 = quantile(stat, p = 0.01)
    )
}

#iterate over datasets
map(x, calc_lower_quantiles)
```

Similarly from what we saw above, the quantiles indicate that the difference in proportions must be larger to be significant if the sample size is small. As the sample size increases, the difference in proportions can be smaller to be significant. 

## p-value

So now we have some understanding of what the distribution should look like when the Null hypothesis is True. The critical regions give us an idea of the extreme ends of the distribution and can be thought of as levels of signficance for the observed data. Now we can finally understand what a p-value is.

When the observed (actual) data falls on the extreme end of the critical value, we can call this disagreement between the observed data and the Null hypothesis the **p-value**. This statistic measures the probability of observing data as or more extreme than the actual value given the Null hypothesis were true. If the p-value is less than critical value (.05 or whatever arbitrary level you set), we must reject the Null hypothesis.

```{r}
cid_perm %>%
  visualize(method = "theoretical") %>%
  shade_p_value(obs_stat = actual, direction = "right")
 
```




